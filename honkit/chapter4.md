# 4. 構文解析アルゴリズム古今東西

　これまで、第2章でJSONを例にしてPEGによる構文解析器と、単純な字句解析器を用いた構文解析器を実装しました。また、第3章でプログラミング言語の構文解析を説明するのに必要な文脈自由文法の概念を紹介しました。そして、ここまで来たようやく準備が整ったので、いよいよ本書の本丸である構文解析アルゴリズムの話が出来ます。

　と言われても、戸惑う読者の方が多いかもしれません。なにせ、これまで「構文解析アルゴリズム」について具体的な話はまったくなかったのでしたから。しかし、皆さんは既に、第2章で**二つ**の構文解析アルゴリズムを使ってJSONの構文解析器を書いているのです。

　多少用語として不正確なのを承知で言うなら、第2章で最初に実装したのは、バックトラックあり再帰下降構文解析器であり、後で実装したのは、LL(1)（っぽい）再帰下降構文解析器と言えます。この、「再帰下降」という言葉が見慣れないものなので、はてなと思われるかもしれませんが、その疑問は一端脇においておいて、第2章での実装が理解出来たなら、皆さんは既に直感的には構文解析アルゴリズムを理解していることになります。

　この章では、2021年8月現在までに発表された主要な（いくつかは筆者の独断と偏見が入っています）構文解析アルゴリズムについて解説していきます。実用的には、紹介する構文解析アルゴリズムのほとんどについて、その構文解析アルゴリズムを使った構文解析器を生成してくれる**構文解析器生成系**が存在するからです。たとえば、おそらく最もメジャーな構文解析器生成系は、`LALR(1)`アルゴリズムを用いたyacc（正確には、GNUによる再実装であるbisonが現在は主流）でしょう。yaccに似た構文解析器生成系はC向けのyaccの他に、Ruby向けのracc、Java向けのJay、OCaml向けのocamlyaccなど数多くのバリエーションがあります。

　他には、`LL(*)`アルゴリズムを用いたANTLRや`LL(k)`アルゴリズムを用いたJavaCCもよく使われています。構文解析器生成系では昔は圧倒的にyaccがメジャーでしたが（個人の感想です）、最近は`LL(k)`アルゴリズムやその拡張もよく見るようになってきました。また、2002年にBryan Fordが発表したPackrat Parsing（正確には、それを形式化したPEG）を用いた構文解析生成系も数多く登場しています。とりわけ、PEGは構文解析器生成系を作るのがとても簡単なこともあって、色々な言語向けの構文解析器生成系が多数公開されています。かくいう筆者も、大学院生時代にPEGおよびPackrat Parsingの研究をしており、その課程で実験用の構文解析器生成系を作ったものでした。

　あまり小難しいことばかり言うのは趣味ではないので、早速、構文解析アルゴリズムの世界を覗いて見ましょう！　

## 4.1 トップダウン構文解析とボトムアップ構文解析

　と言ったばかりなのに大変恐縮なのですが、具体的な構文解析アルゴリズムの解説に入る前に、構文解析アルゴリズムは大別して、

- 上から下へ（トップダウン）
- 下から上へ（ボトムアップ）

　の二つのアプローチがあることを理解しておきましょう。といっても、難しい話ではありません。

　まずはトップダウン構文解析法です。第3章で例に出てきたDyck言語の文法は以下のようなものでした。

```bnf
D = P
P = "(" P ")" | ""
```

　このBNFは、カッコが正しくネストした文字列を過不足無く表現しているわけですが、このBNFを元にして自力で構文解析器を作るにはどうすればいいか考えてみましょう。おそらく、まず皆さんが素朴に思いつくのは以下のような実装ではないかと思います。

```java
public class Dyck {
    public boolean D () {
        P();
    }
    public boolean P() {
        while(hasNext()) {
            String first = peekToken();
            if(first.equals("("))) {
                nextToken();
                P();
                String end = nextToken();
                return end.equals(")");
            } else {
                return true;
            }
        }
    }
}
```

　このプログラムあるいはクラス`Dyck`はまさに`Dyck`言語を構文解析して、成功したなら`true`、そうでなければ`false`を返すものです。BNFと比較すると、

- 規則の名前と一対一になる関数が存在する
- 非終端記号への参照は規則の名前に対応する関数の**再帰呼び出し**として実現されている

のが特徴です。呼び出す規則を上、呼び出される規則を下とした時、上から下に再帰呼び出しが続いていくため再帰下降構文解析と呼ばれます。このような、「上から下に」構文解析を行っていくのがトップダウン型構文解析法の特徴です。注意しなければいけないのは、上から下へ解析を行うアルゴリズムは多数あり、その一つに再帰下降構文解析があるということです。その他のトップダウン型構文解析法については後々紹介していきますのでご安心ください。

　一方、ボトムアップ構文解析法はその逆です。といっても、こちらの方法はトップダウン型より直感的に理解しづらいかもしれません。ボトムアップ構文解析法の直感的なイメージは、「右から左にブロックが順番に降って来て、BNFの右辺にマッチした記号列があれば、左辺の規則名にマッチしたとして、還元動作をおこなうこと」です。

　たとえば、`(())`という文字列を解析することを考えてみましょう。ボトムアップ解析では、まず、最初の「1文字」を右から左にシフトします。ちょうど以下のようなイメージです。

```bnf
「(」
```

　このブロックとルールPの右辺はマッチしないので、もう1文字をシフトしてみます。

```bnf
「（」「(」
```

　まだマッチしない……本当にそうでしょうか？しかし、よく見ると規則Pの右辺には空文字列`"`も含まれているのでした。ということは、空のブロック「」とマッチしたことにして、以下のように考えても問題ないことになります。

```bnf
「（」「（」P
```

　さらに、1文字シフトすると、

```bnf
「（」「（」P「）」
```

　となります。これは、Pの右辺にマッチするので、以下のように還元出来そうです。

```bnf
「（」P「）」
```

　さらに、同様にしてこれはPの右辺にマッチするので、最終的にPに還元されます：

```bnf
P
```

　そして、Dの右辺はPなので、

```bnf
D
```

となり、めでたく最終結果であるDの構文解析に成功しました。このように、

1. とりあえず、文字を右から左に引っ張って、積み重ねる（シフト）
2. 文法を参照して、右辺が積み重なった記号列にマッチしたら、左辺の記号に置き換える（還元）

を繰り返し行くアプローチをボトムアップ構文解析法を呼びます。トップダウン型の構文解析アルゴリズムが多数あるように、ボトムアップ型の構文解析アルゴリズムも多数あります。一番著名であると思われるyaccが採用しているLALR(1)もボトムアップ型の構文解析アルゴリズムです。

ちなみに、トップダウン型とボトムアップ型にはそれぞれ異なった利点と欠点があります。特に、トップダウン型は規則と関数を対応付けるのが容易なので手書きの構文解析器を書くのに向いて居ますし、関数の引数として情報を渡して、それに応じて処理を分岐させるといったことが得意です。一方で、トップダウン型は左再帰に弱いという欠点があります。たとえば、以下のBNFは、ボトムアップ型だと`a*`に相当する言語を普通に解析出来ますが、工夫なしにトップダウン型で実装すると無限再帰に陥ってスタックオーバーフローします。

```
A = A "a"
  | "";
```

もちろん、このような問題をトップダウン型で解決する方法も存在します。端的に言うと、「再帰をループに置き換える」といったもので、BNFを

```
A = "a" A
  | "";
```

このように書き換えればトップダウン型でも問題なく解析出来るようになります。しかし、ならボトムアップ型が一方的に有利なのかというと事態はそう単純ではないのが面白いところです。特に、前後の文脈に応じて構文解析方法を切り替えるのは比較的ボトムアップ型だとやりづらいところです（出来ない訳ではないです）。

## 4.2 LL(1) - 代表的なトップダウン構文解析アルゴリズム

構文解析アルゴリズムの中でおそらくもっとも古典的で、よく知られているのは`LL(1)`構文解析アルゴリズムです。なんか一見小難しく見えますよね。しかし、`LL(1)`のイメージというのは意外に簡単なものです。

たとえば、以下のようなJava言語のif文があったとします。

```java
if(age < 18) {
    System.out.println("18歳未満です");
} else {
    System.out.println("18歳以上です");
}
```

非常に簡単ですよね。しかし、我々は如何にしてこれを見て「if文がある」と認識するのでしょうか。もちろん「人それぞれ」なのですが、最初に`if`が現れたからif文だと考える人も多いのではないかと思います。

`LL(1)`構文解析アルゴリズムはまさにこのイメージを元にした手法です。プログラムをトークン列に区切った後に、「最初の1トークン」を見て、「あ、これはif文だ」とか「あ、これはwhile文だ」とか認識するようなものですね。

ただし、イメージとしては簡単なのですが、アルゴリズムとして実行可能なようにするためには考えなければいけない論点がいくつかあります。以下では、`LL(1)`を実装するに当たって考えなければいけない課題について論じてみます。

### 課題1 - ある構文の最初のトークンが複数種類ある場合

先程の例ではある構文、たとえばif文が始まるには`if`というキーワードが必須で、それ以外の方法でif文が始まることはありえませんでした。しかし、たとえば、算術式を考えてみると、問題はそう単純ではないことがわかります。少し考えただけでも、以下のような例が思い浮かびます。

- `(`で算術式が始まる場合
- `-`で算術式が始まる場合
- `+`で算術式が始まる場合
- 整数リテラル（`<integer_literal>`）で算術式が始まる場合
- 浮動小数点数リテラル（`<floating_point_literal>`）で算術式が始まる場合

つまり、次のトークンが算術式の始まりである事を確定するためには、トークンの集合という概念が必要になります。たとえば、算術式の始まりは

```
{"(", "-", "+", <integer_literal>, <floating_point_literal>, ...}
```

のようなトークンの集合であると考える事が出来ます。このような、ある構文が始まるかを決定するために必要なトークンの集合のことを**FIRST集合**と呼びます。

### 課題2 - 省略可能な要素の扱い

if文の例は、次の1トークンを見ればどんな構文かわかる例でした。しかし、if文にelseが出てこない場合に、if文からif-else文か正確に決定するにはどうすればいいのでしょうか？たとえば、以下の文は正当です。

```java
if(age < 18) {
    System.out.println("18歳未満です");
};
if(age >= 18) {
    System.out.println("18歳未満です");
}
```

他方で以下の文も正当です。

```java
if(age < 18) {
    System.out.println("18歳未満です");
}
if(age >= 18) {
    System.out.println("18歳未満です");
}
```

最初のif文の後に、

- 前者は`;`が出現する
- 後者は`if`が出現する

という違いがあるわけですが、どちらの場合にしても、それらが出現した時点で最初のif文が終わるのは明らかでしょう。以下の場合、`System`が出現した時点で最初のif文が終了したことはわかります。

```java
if(age < 18) {
    System.out.println("18歳未満です");
}
System.out.println("終了します");
```

まとめると、

- 直後に`else`が出現すればif-else文である
- それ以外で、かつ構文エラーにならないあらゆるトークンが出てきた場合、if文である

ということが出来そうです。しかし「あらゆるトークン」と言われても漠然としていて、アルゴリズムとしては不完全です。アルゴリズムとしては、以下のようになっていなければ困ります。

- 直後に`else`が出現すればif-else文である
- トークン `{";", <identifier>, "if", ...}`が出てきた場合、if文である

`LL(1)`をきちんと考えようとすると、「省略可能なトークンの次のトークンの集合」について考える必要があるわけです。このような「～の次のトークンの集合」も、ある構文がどう始まるかを決定するのに必要になります。これを**FOLLOW集合**と呼びます。

先の項で出てきた**FIRST集合**と**FOLLOW集合**は`LL(1)`にとって重要な概念です。次の項以降では、この**FIRST集合**と**FOLLOW集合**の概念についてより厳密に説明します。

### FIRST集合とFOLLOW集合の計算

`LL(1)`をアルゴリズムとしてきちんと定義しようとするなら、この二つの概念が必要であることはわかってもらえたと思います。しかし、この二つですが、一体プログラム上でどう計算すれば良いのでしょうか？この問いに答える事が`LL(1)`アルゴリズムをきちんと理解する事であり、逆にきちんと理解出来れば、自力で`LL(1)`アルゴリズムによるパーザを記述出来るようになるでしょう。

まずはFIRST集合について考えてみます。(WIP)

次にFOLLOW集合について考えてみます。(WIP)

## LL(1)の問題点と限界

`LL(1)`は古典的でありかつそれなりに実用的でもありますが、アルゴリズムがシンプルである故の問題点や限界も存在します。この後では`LL(1)`の抱える問題点について述べます。

### 問題点1 - 最初の1トークンで構文要素を決められないことがある

先の節では明らかに最初の1トークンで「これはif文」とか「これはwhile文」とか決められる場合のみを対象にしてきました。しかし、現実にはそれだけではどうにもならない場合があります。

たとえば、Java 7で導入されたダイヤモンド演算子について考えてみます。以下のようにして、ジェネリクスの型パラメータ指定を省略出来る機能です。

```java
List<Person> people;
person = new ArrayList<>();
```

2行目の右辺を仮にBNFにすると以下のようになるでしょうか。

```bnf
new_object = new id ("<" ... ">" /* 普通のnew演算子 */ | "<" ">" /* ダイヤモンド演算子 */) params
```

ここで、右辺の`new ArrayList<>`について注目してみましょう。`<`はJava言語ではそれだけで一つのトークンですが、`ArrayList`の直後にある`<`を見ただけではダイヤモンド演算子を使っていることはわかりません。これは次の例をみるとわかります。

``java
List<Person> people;
person = new ArrayList<Person>();
```

上の例を見ればわかる通り、`ArrayList<`を読んだ時点では、`new ArrayList<>()`の可能性も`new ArrayList<Person>()`の可能性のどっちも考えられるのです。

この問題は、`<`が出現した時点では、ダイヤモンド演算子かどうかは決めずに、次のトークンが`>`ならダイヤモンド演算子、そうでないなら通常のインスタンス生成とする回避方法が使えます。BNFにすると以下のようになるでしょうか。

```bnf
new_object = "new" id type_params params
type_params = "<" type_params_suffix
type_params_suffix  = ">" | id ("," id)* ">" // ">"が来たらダイヤモンド演算子、そうでないなら通常のインスタンス生成 

このような、「先頭の1トークン」をくくりだす方法をleft factoringと呼びます。経験上、`LL(1)`の制限の多くはleft factoringによって回避することが出来ます。

### 問題点2 - left factoringで変形しても次の構文要素を決められない場合

問題点1は、最初の1トークンで次の構文要素を決められないというものでした。前述したようにこの点についてはleft factoringによって容易に解決する事が可能ですが、それでも次の構文要素を決められない場合もあります。例えば、次のような変数宣言を考えます。

```java
hogep.foop.barp.Piyo piyo;
```

この時、先頭にあるパッケージ名の部分はいくらでも長くなる可能性があります。さらに、型宣言を読んでいる時点では、実は以下のようにメソッド呼び出しであったという可能性は捨てきれません。

```java
hogep.foop.barp.Piyo()
```

これはleft factoringだけでは簡単に解決出来ません（解決が不可能とまでは証明できませんが困難です）。先頭の共通部分をくくりだそうにも、共通部分の長さが不明確なのですから。この問題は割と本質的なものなので、実用上は構文解析の後か前に処理を追加して解決することが多いです。たとえばドット（`.`）で繋がった名前の連なりを`fqn`という規則でくくり出すという手段を使うことが出来ます。この手法には特に名前はついていませんが、構文解析の先送りとでも呼ぶのがいいかもしれません。

```java
fqn = id ("." id)*
```

本質的にはこのようなパターンは文脈依存言語を扱っているという事ができ、CFGでもPEGでも取り扱いがやや難しいものです。さしあたって、一見構文解析の問題に見える事は必ずしも構文解析の段階で解決出来るわけではないということを認識しておいてください。

## 4.3 LL(k) - LL(1)の拡張

前節で紹介したLL(1)アルゴリズムには次のような欠点がありました。

1. 1トークン先読みでは出現する構文要素を決定できない場合がある
2. left factoringを行ってもなお1トークン先読みでは出現する構文要素を決定できない場合がある

この二つの欠点の内、前者を解決するのがLL(k)アルゴリズムです。kは1以上の整数で、具体的な数値を決定することでアルゴリズムが決まります。

たとえば、2トークン先読みするのはLL(2)、3トークン先読みするのはLL(3)といった具合です。さて、とにもかくにも具体例です。わかりやすいように、LL(1)アルゴリズムでは解析できないがLL(2)アルゴリズムでは解析できる例を考えてみます。もちろん、そのような文法はほぼ常にleft factoringすることでLL(1)で解析可能なように変形可能ですが、プログラマがそのような作業をするのは手間ですから、あらかじめ構文解析器が適切に先読みしてくれればそれに越したことはないでしょう。


少し恣意的な例ですが、CやJava風味の型宣言を持った言語を考えてみます。ただし、議論しやすいようにするために、型名は必ず1トークンになるものとします。また、CやJavaのように式文があるとします。そのような言語で、型宣言`user_type x;`と式文`foo;`を区別するにはどうすれば良いでしょうか？

まず、明らかにLL(1)では二つを区別することができません。なぜなら、最初のトークンが`user_type`だったとして果たしてこれが型名なのか変数名なのかを区別することは出来ないからです。もちろん、型名のテーブルを持っていれば可能ですがそれでは文脈自由言語の範囲を逸脱してしまいます。

ここで、LL(1)ではなくLL(2)アルゴリズムを使うとしてみます。すると、`user_type x`という二つのトークンが来た場合は明らかに型宣言であることがわかありますし、`foo ;`という二つのトークンが来た場合は式文であることがわかります。

つまり、先読みトークン数を1から2に増やすことでより広い範囲の文法を認識できるようになったわけです。

さらに、kは任意の整数で良いのですから、kを3に増やしても4に増やしてもいいわけです。後述するようにkの数を増やし過ぎることは消費する記憶領域や実行性能面での問題があるのですが、それをひとまずおいておけばkを増やすことでより広い範囲の文法を認識できるようになっていく、と言えるわけです。

## 4.4 LL(k)の限界

LL(k)ではkの数を増やしていくことで表現能力を拡張できることがわかったわけですが、ここで一つの疑問が出てきます。LL(k)のkをいくらでも増やしていくことによって、任意の文脈自由言語を認識できるように拡張可能なのだろうか、というものです。この疑問に答えるためには複雑な議論が必要ですが、端的に答えだけをいうとNOです。文脈自由言語であるがLL(k)では認識できない言語は必ず存在します。

たとえば、a^i b^j （i >= j >= 1）、つまりaがi回あらわれてその後にbがj回（i回以上）あらわれるような言語はLL(k)言語ではありません（後述しますが、LR(1)言語ではあります）。

## 4.5 LR Parsing - ボトムアップ構文解析アルゴリズムの基本

　さて、ここまででLL(1)からはじまるトップダウン型構文解析アルゴリズムについて解説してきました。トップダウン型構文解析アルゴリズムは人間の直感に合致してることやエラーメッセージの出しやすさなどの利点があるのですが、悲しいことに素朴なLL(k)アルゴリズムでは表現力に限界があります。そのようなこともあって、構文解析の世界ではより表現力の高いLR(1)アルゴリズムあるいはその変種が広く使われています。ただし、そのような認識が支配的だったのは2000年代はじめ頃までであって、現代ではLL(k)をより拡張したLL(*)やALL(*)を実装したANTLRなどが広く使われていますし、ある意味ではPEGもトップダウン型のアルゴリズムという意味では「お仲間」とも言えます。ともあれ、LR(1)アルゴリズムの変種が構文解析の世界で広く使われているのも間違いありません。

　たとえば、この本をお読みの読者ならyacc(その互換製品であるbison。以後yaccで統一します)という名前を一度は聞いたことがあるかもしれません。yaccはもっとも広く使われている構文解析器生成系であり、LALR(1)アルゴリズムを採用しています。yaccは[Rubyの構文解析器](https://github.com/ruby/ruby/blob/db6b23c76cbc7888cd9a9912790c2068703afdd0/parse.y)を作るためなど、言語処理系の構文解析器を生成するために使われています。

　LR parsingはTexで有名なDonald Knuthによって提案されました。LR parsingの基本的なアイデアは次のようなものです。

TBD

## 4.6 LALR(1) - 現実的に取り扱いやすいボトムアップ構文解析アルゴリズム

　「素の」LR parsingは非常に幅広い範囲の文法を取り扱うことができますが、構文解析表が巨大化してしまうなどの問題があります（2020年代に入った現在では必ずしも当てはまらないかもしれません）。多くの構文解析生成系ではLR parsingの変種であるLALR法、特に先読み数が1であるLALR(1) parsingが利用されています。

TBD

## 4.7 - LR(k)の限界

　LR(1) parsingで受理可能な言語とLR(k) parsingで受理可能な言語は等しいことが知られています。つまり、原理的にはLR(1) parsingがあればLR(k)言語を受理可能であり、LR(k)言語はDCFL（決定的文脈自由言語）とも等しいです。これはLR(k)法が非常に強力なアルゴリズムであることを示唆していますが、LR(k)言語でも表現できない文脈自由言語が存在しています。

TBD（コメント：書いていて、ここはちょっと難しいなと実感。というのは、DCFLでない言語で文脈自由というのは自然言語処理みたいな曖昧性を許す文脈であれば意味があっても、そうでない文脈であれば大差ないとも言えるので）

## 4.8 - Parsing Expression Grammar(PEG) - 構文解析アルゴリズムのニューカマー

　2000年代に入るまで、構文解析手法の主流はLR法の変種であり、つまるところボトムアップ構文解析アルゴリズムでした。その理由の一つに、先読みを前提とする限り、従来のLL法はLR法より表現力が弱いという弱点がありました。トップダウン型でもバックトラックを用いればより幅広い言語を表現できることは比較的昔から知られていましたが、バックトラックによって解析時間が最悪で指数関数時間になるため、コンパイラの教科書として採用されることが多い、いわゆるドラゴンブックでも現実的ではないといった記述がありました（記憶に頼ったもので、出典が曖昧。現在のドラゴンブックにも該当記述があるかは要確認）。しかし、2004年に提案されたParsing Expression Grammar（PEG）はそのような状況を変えました（Ford:2004）。

　PEGはおおざっぱに言ってしまえば、無制限な先読みとバックトラックを許すトップダウン型の構文解析手法の一つです。DCFL＋一部の文脈依存言語を取り扱うことができますし、その上、Packrat Parsingという手法によって線形時間で構文解析を行うことが保証されているというとても良い性質を持っています。さらに、実質的にLL法やLR法で必須であった字句解析器と構文解析器の分離が要らず、アルゴリズムも非常にシンプルであるため、ここ十年くらいで解析表現手法をベースとした構文解析生成系が数多く登場しました。

　他人事のように書いていますが、ほかならぬ筆者が大学院時代に専門分野として研究していたのがまさしくこのPEGでした。さらに、Python 3.9ではPEGベースの構文解析器が採用されるなど、着々と採用されるケースが増えています。

　2章で既にPEGを用いた構文解析器を自作したのを覚えているでしょうか。たとえば、配列の文法を表現した以下のPEGがあるとします。

```bnf
array = LBRACKET RBRACKET | LBRACKET {value {COMMA value}} RBRACKET ;
```

　このPEGに対応するJavaの構文解析器（メソッド）は以下のようになるのでした。

```java
    public Ast.JsonArray parseArray() {
        int backup = cursor;
        try {
            // LBRACKET RBRACKET
            parseLBracket();
            parseRBracket();
            return new Ast.JsonArray(new ArrayList<>());
        } catch (ParseException e) {
            cursor = backup;
        }

        // LBRACKET
        parseLBracket();
        List<Ast.JsonValue> values = new ArrayList<>();
        // value
        var value = parseValue();
        values.add(value);
        try {
            // {value {COMMA value}}
            while (true) {
                parseComma();
                value = parseValue();
                values.add(value);
            }
        } catch (ParseException e) {
            // RBRACKET
            parseRBracket();
            return new Ast.JsonArray(values);
        }
    }
```

　PEGの特色は、

```java
        int backup = cursor;
```

　という行によって、解析を始める時点でのソースコード上の位置を保存しておき、もし解析に失敗したら以下のように「巻き戻す」ところにあります。そして、「巻き戻した」位置から次の分岐を試そうとするのです。

```java
        } catch (ParseException e) {
            cursor = backup;
        }
        // LBRACKET
        parseLBracket();
        // ...
```

　なお、PEGの挙動を簡単に説明するために2章および本章では例外をスロー/キャッチするという実装にしていますが、現実にはこのような実装にするとオーバーヘッドが大きすぎるため、実用的なPEGパーザでは例外を使わないことが多いです。

　より一般化すると、PEGの挙動は以下の8つの要素を使って説明することができます。

1. 空文字列： ε
2. 終端記号： t
3. 非終端記号： N
4. 連接： e1 e2
5. 選択： e1 / e2
6. 0回以上の繰り返し： e*
7. 肯定述語： &e
8. 否定述語： !e
  
  次節以降では、この8つの要素がそれぞれどのような意味を持つかを説明していきます。

### 空文字列

TBD

### 終端記号

TBD

### 非終端記号

　あるPEGの規則Gがあったとき、

```peg
G <- H e1
H <- e2
```

1. Hに対応する規則を探索する（H <- eが該当）
2. Hの呼び出しから戻って来たときのために、スタックに現在位置を退避
3. e2とsの照合を行う
4. 3.が失敗した場合、H e1全体が失敗する
5. 3.が成功した場合、e1とsのサフィックスを照合した結果を返す

### 連接

　あるPEGの規則Gがあったとき、

```peg
G <- e1 / e2
```

　Gと文字列sの照合を行うために、以下のような動作を行います。

1. e1とsの照合を行う
2. 1.が成功していれば、sのサフィックスを返し、成功する
3. 1.が失敗した場合、e2と2の照合を行い、結果を返す

### 選択

　あるPEGの規則Gがあったとき、

```peg
G <- e1 e2
```

　Gと文字列sの照合を行うために、以下のような動作を行います。

1. e1とsの照合を行う
2. 1.が成功していれば、e2とsのサフィックスの照合を行い結果を返す
3. 1.が失敗した場合、その結果を返す

### 0回以上の繰り返し

　あるPEGの規則Gがあったとき、

```peg
G <- e*
```

　Gと文字列eの照合を行うために、以下のような動作を行います。

1. eとsの照合を行う
2. 1.が成功していれば、sをsのサフィックスに置き換えて、1に戻る
3. 1.が失敗した場合、sを返し、成功する

　`e*`は「0回以上の繰り返し」を表現するため、一回も成功しない場合でも、全体が成功するのがポイントです。

### 肯定述語

TBD

### 否定述語

TBD

### PEGの操作的意味論

ここまでで、PEGを構成する8つの要素について説明してきましたが、実際のところやや厳密さに欠けるものでした。この挙動をより厳密に説明すると以下のようになります。

（以下、Fordの論文をはりつけたもの。これをベースに日本語の説明に書き換える）

```
1. Empty: 
  (ε,x) ⇒ (1,ε) for any x ∈ V ∗ T.
2. Terminal (success case): 
  (a,ax) ⇒ (1,a) if a ∈VT , x ∈V ∗ T.
3. Terminal (failure case):
   (a,bx) ⇒ (1, f) if a 6= b, and (a,ε) ⇒ (1, f).
4. Nonterminal:
  (A,x) ⇒ (n + 1,o) if A ← e ∈ R and (e,x) ⇒ (n,o).
5. Sequence (success case): 
  If (e1,x1x2y) ⇒ (n1,x1) and (e2,x2y) ⇒ (n2,x2), then (e1e2,x1x2y) ⇒ (n1 +n2 +1,x1x2).  Expressions e1 and e2 are matched in sequence, and if each succeeds and consumes input portions x1 and x2 respectively, then the sequence succeeds and consumes the string x1x2.
6. Sequence (failure case 1): 
  If (e1,x) ⇒ (n1, f), then (e1e2,x) ⇒ (n1 + 1, f). If e1 is tested and fails, then the sequence e1e2 fails without attempting e2, 
7. Sequence (failure case 2): 
  If (e1,x1y) ⇒ (n1,x1) and (e2,y) ⇒ (n2, f), then (e1e2,x1y) ⇒ (n1 + n2 + 1, f). If e1 succeeds but e2 fails, then the sequence expression fails.
8. Alternation (case 1): 
  If (e1,xy) ⇒ (n1,x), then (e1/e2,xy) ⇒ (n1 +1,x). Alternative e1 is first tested, and if it succeeds, the expression e1/e2 succeeds without testing e2.
9. Alternation (case 2): 
  If (e1,x) ⇒ (n1, f) and (e2,x) ⇒ (n2,o), then (e1/e2,x) ⇒ (n1 + n2 + 1,o). If e1 fails, then e2 is tested and its result is used instead.
10. Zero-or-more repetitions (repetition case): 
  If (e,x1x2y) ⇒ (n1,x1) and (e∗,x2y) ⇒ (n2,x2), then (e∗,x1x2y) ⇒ (n1 + n2 +1,x1x2).
11. Zero-or-more repetitions (termination case): 
  If (e,x) ⇒ (n1, f), then (e∗,x) ⇒ (n1 +1,ε).
12. Not-predicate (case 1): 
  If (e,xy) ⇒ (n,x), then (!e,xy) ⇒ (n + 1, f). If expression e succeeds consuming input x, then the syntactic predicate !e fails.
13. Not-predicate (case 2): 
  If (e,x) ⇒ (n, f), then (!e,x) ⇒ (n + 1,ε). If e fails, then !e succeeds but consumes nothing.
```

## 4.9 - Packrat Parsing

　「素の」PEGは非常に単純かつ、とても幅広い範囲の言語を取り扱うことができます。しかし、PEGには一つ大きな弱点があります。解析時間が最悪の場合に指数関数時間になってしまうことです。現実的にはそのようなケースは稀であるという指摘も複数の論文でなされていますが、原理的にはそのような弱点があります。Packrat Parsingはメモ化という技法を用いることでPEGで表現される言語を線形時間で解析可能にします。

　メモ化という技法自体をご存じでない読者の方も多いでしょうから、ここではまずメモ化について説明します。

### 4.9.1 fibメソッド

　メモ化の例で一番（筆者調べ）出てくるのはN番目のフィボナッチ数を求める`fib`関数です。この書籍をお読みの皆様ならお馴染みかもしれませんが、N番目のフィボナッチ数F(n)は次のようにして定義されます：

```
F(0) = 1
F(1) = 1
F(n) = F(n - 1) + F(n - 2)
```

　このような再帰的な定義を素朴にJavaのメソッドとして書き下したのが以下のfibメソッドになります。

```java
public class Main {
    public static long fib(long n) {
        if(n == 0 || n == 1) return 1L;
        else return fib(n - 1) + fib(n - 2); 
    }
    public static void main(String[] args) {
        System.out.println(fib(5)); // 120
    }
}
```

　このプログラムを実行すると、コメントにある通り120が出力されます。しかし、このfibメソッドには重大な欠点があります。それは、nが増えると計算量が指数関数的に増えてしまうことです。たとえば、上のfibメソッドを使うと`fib(30)`くらいまではすぐに計算することができます。しかし、`fib(50)`を求めようとすると皆さんのマシンではおそらく数十秒はかかるでしょう。

　たかだかフィボナッチ数を求めたいだけなのに数十秒もかかってはたまったものではありません。

### 4.9.2 fib関数のメモ化

　そこで出てくるのがメモ化というテクニックです。一言でいうと、メモ化とはある引数nに対して計算した結果f(n)をキャッシュしておき、もう一度同じnに対して呼び出されたときはキャッシュした結果を返すというものです。早速、fibメソッドをメモ化してみましょう。

　メモ化されたfibメソッドは次のようになります。

```java
import java.util.*;
public class Main {
    private static Map<Long, Long> cache = new HashMap<>();
    public static long fib(long n) {
        Long value = cache.get(n);
        if(value != null) return value;

        long result;
        if(n == 0 || n == 1) {
            result = 1L;
        } else {
            result = fib(n - 1) + fib(n - 2);
        }
        cache.put(n, result);
        return result;
    }
    public static void main(String[] args) {
        System.out.println(fib(50)); // 20365011074
    }
}
```

　`fib(50)`の結果はコメントにある通りですが、今度は一瞬で結果がかえってきたのがわかると思います。メモ化されたfibメソッドでは同じnに対する計算は二度以上行われないので、nが増えても実行時間は線形にしか増えません。つまり、fib(50)の実行時間は概ねfib(25)の二倍であるということです。

　ただし、計算量に詳しい識者の方は「おいおい。整数同士の加算が定数時間で終わるという仮定はおかしいんじゃないかい？」なんてツッコミを入れてくださるかもしれませんが、そこを議論するとややこしくなるので整数同士の加算はたかだか定数時間で終わるということにします。

　`fib`メソッドのメモ化でポイントとなるのは、記憶領域（`cache`に使われる領域）と引き換えに最悪計算量を指数関数時間から線形時間に減らせるということです。また、メモ化する対象となる関数は一般的には副作用がないものに限定されます。というのは、メモ化というテクニックは「同じ引数を渡せば同じ値が返ってくる」ことを暗黙の前提にしているからです。

　次の項ではPEGをナイーヴに実装した`parse`関数をまずお見せして、続いてそれをメモ化したバージョン（Packrat parsing）をお見せすることにします。`fib`メソッドのメモ化と同じようにPEGによる構文解析もメモ化できることがわかるでしょう。

### 4.9.3 parseメソッド

　ここからは簡単なPEGで記述された文法を元に構文解析器を組み立てていくわけですが、下記のような任意個の`()`で囲まれた`0`の構文解析器を作ります。

```
A <- "(" A ")"
   / "0"
```

　一見単純過ぎる例にも思えますが、メモ化の効果を体感するにはこれで十分とも言えるのです。さて、早速構文解析器を書いていきましょう。

```java
sealed interface ParseResult permits ParseResult.Success, ParseResult.Failure {
    public abstract String rest();
    record Success(String value, String rest) implements ParseResult {}
    record Failure(String rest) implements ParseResult {}
}
class ParseError extends RuntimeException {
    public final String rest;
    public String rest() {
        return rest;
    }
    ParseError(String rest) {
        this.rest = rest;
    }
}
public class Parser {
    private static boolean isEnd(String string) {
        return string.length() == 0;
    }
    public static ParseResult parse(String input) {
        try {
            // "(" A ")"
            if(isEnd(input) || input.charAt(0) != '(') {
                throw new ParseError(input);
            }

            var result = parse(input.substring(1));
            if(!(result instanceof ParseResult.Success)) {
                throw new ParseError(result.rest());
            }

            var success = (ParseResult.Success)result;

            if(isEnd(success.rest()) || success.rest().charAt(0) != ')') {
                throw new ParseError(success.rest());
            }

            return new ParseResult.Success(")", success.rest().substring(1));
        } catch (ParseError error) {
            if((!isEnd(error.rest())) && error.rest().charAt(0) == '0') {
                return new ParseResult.Success("0", error.rest().substring(1));
            }
            return new ParseResult.Failure(error.rest());
        }
    }
}
```

このプログラムを使うと以下のように構文解析を行うことが出来ます。

```java
jshell> Parser.parse("(");
$25 ==> Failure[rest=]
jshell> Parser.parse("()");
$26 ==> $26 ==> Failure[rest=)]
jshell> Parser.parse("(0)");
$27 ==> Success[value=), rest=]
```
　
しかし、この構文解析器には弱点があります。

<!--
考えてみるとこの文法だと指数関数時間にならない……
-->

### 4.9.4 parseメソッドのメモ化 - Packrat Parsing

TBD

## 4.10 - Generalized LR (GLR) Parsing

Generalized LR(GLR) parsingはTomitaらによって1991年に提案された手法です（Tomita:1991）。Generalized LRという名前の通り、非決定的で曖昧なCFGを取り扱えるようにLR parsingを拡張したアルゴリズムです。

TBD

## 4.11 - Generalized LL (GLL) Parsing

Generalized LL(GLL) parsingはScottらによって2010年に提案された手法です（Scott:2010）。Generalizd LLという名前の通りLL parsingを拡張したものですが、先行するGLR parsingの影響を受けたアルゴリズムです。

TBD

## 4.12 - Parsing with Derivatives (PwD)

Parsing with derivatives(PwD)はMightらによって2011年に提案された手法です（Might:2011）。

## 4.13 - 構文解析アルゴリズムの計算量と表現力の限界 

　LL parsing、LR parsing、PEG、Packrat parsing、GLR parsing、GLL parsingについてこれまで書いてきましたが、計算量的な性質についてまとめておきましょう。

- LL(k)
- LR(k)
- PEG
- Packrat pasing
- GLR parsing
- GLL parsing
- PwD

## 4.14 - まとめ

　この章では構文解析アルゴリズムの中で比較的メジャーな手法について、そのアイデアと概要を含めて説明しました。その他にも多数の手法がありますが、いずれにせよ、「上から下に」向かって解析するトップダウンの手法と「下から上に」向かって解析するボトムアップの手法のどちらかに分類できると言えます。

　GLRやGLL、PwDについては普段触れる機会はそうそうありませんが、LLやLR、PEGのパーザジェネレータは多数存在するため、基本的な動作原理についておさえておいて損はありません。また、余裕があれば各構文解析手法を使って実際のパーザジェネレータを実装してみるのも良いでしょう。実際にパーザジェネレータを実装することで、より深く構文解析手法を理解することもできます。